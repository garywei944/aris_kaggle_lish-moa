{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading data\n",
    "\"\"\"\n",
    "df_train = pd.read_csv('cs589_mini_project/data/train_features.csv')\n",
    "df_test = pd.read_csv('cs589_mini_project/data/test_features.csv')\n",
    "df_target_s = pd.read_csv('cs589_mini_project/data/train_targets_scored.csv')\n",
    "submission = pd.read_csv('cs589_mini_project/data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 876)\n",
      "(3982, 876)\n",
      "(23814, 207)\n",
      "(3982, 207)\n"
     ]
    }
   ],
   "source": [
    "# display(df_train.head(5))\n",
    "# display(df_test.head(5))\n",
    "# display(df_target_s.head(5))\n",
    "# display(submission.head(5))\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_target_s.shape)\n",
    "print(submission.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>cp_type</th>\n",
       "      <th>cp_time</th>\n",
       "      <th>cp_dose</th>\n",
       "      <th>g-0</th>\n",
       "      <th>g-1</th>\n",
       "      <th>g-2</th>\n",
       "      <th>g-3</th>\n",
       "      <th>g-4</th>\n",
       "      <th>g-5</th>\n",
       "      <th>...</th>\n",
       "      <th>c-90</th>\n",
       "      <th>c-91</th>\n",
       "      <th>c-92</th>\n",
       "      <th>c-93</th>\n",
       "      <th>c-94</th>\n",
       "      <th>c-95</th>\n",
       "      <th>c-96</th>\n",
       "      <th>c-97</th>\n",
       "      <th>c-98</th>\n",
       "      <th>c-99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_000644bb2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0620</td>\n",
       "      <td>0.5577</td>\n",
       "      <td>-0.2479</td>\n",
       "      <td>-0.6208</td>\n",
       "      <td>-0.1944</td>\n",
       "      <td>-1.0120</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2862</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.8076</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.1912</td>\n",
       "      <td>0.6584</td>\n",
       "      <td>-0.3981</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>0.3801</td>\n",
       "      <td>0.4176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_000779bfc</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0743</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.0604</td>\n",
       "      <td>1.0190</td>\n",
       "      <td>0.5207</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4265</td>\n",
       "      <td>0.7543</td>\n",
       "      <td>0.4708</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.4899</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>0.6077</td>\n",
       "      <td>0.7371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_000a6266a</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6280</td>\n",
       "      <td>0.5817</td>\n",
       "      <td>1.5540</td>\n",
       "      <td>-0.0764</td>\n",
       "      <td>-0.0323</td>\n",
       "      <td>1.2390</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.7250</td>\n",
       "      <td>-0.6297</td>\n",
       "      <td>0.6103</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>-1.3240</td>\n",
       "      <td>-0.3174</td>\n",
       "      <td>-0.6417</td>\n",
       "      <td>-0.2187</td>\n",
       "      <td>-1.4080</td>\n",
       "      <td>0.6931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_0015fd391</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.5138</td>\n",
       "      <td>-0.2491</td>\n",
       "      <td>-0.2656</td>\n",
       "      <td>0.5288</td>\n",
       "      <td>4.0620</td>\n",
       "      <td>-0.8095</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.0990</td>\n",
       "      <td>-0.6441</td>\n",
       "      <td>-5.6300</td>\n",
       "      <td>-1.3780</td>\n",
       "      <td>-0.8632</td>\n",
       "      <td>-1.2880</td>\n",
       "      <td>-1.6210</td>\n",
       "      <td>-0.8784</td>\n",
       "      <td>-0.3876</td>\n",
       "      <td>-0.8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_001626bd3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.3254</td>\n",
       "      <td>-0.4009</td>\n",
       "      <td>0.9700</td>\n",
       "      <td>0.6919</td>\n",
       "      <td>1.4180</td>\n",
       "      <td>-0.8244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.6670</td>\n",
       "      <td>1.0690</td>\n",
       "      <td>0.5523</td>\n",
       "      <td>-0.3031</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>0.2885</td>\n",
       "      <td>-0.3786</td>\n",
       "      <td>0.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>id_fffb1ceed</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1394</td>\n",
       "      <td>-0.0636</td>\n",
       "      <td>-0.1112</td>\n",
       "      <td>-0.5080</td>\n",
       "      <td>-0.4713</td>\n",
       "      <td>0.7201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1969</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>-0.8121</td>\n",
       "      <td>0.3434</td>\n",
       "      <td>0.5372</td>\n",
       "      <td>-0.3246</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.9171</td>\n",
       "      <td>0.5258</td>\n",
       "      <td>0.4680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>id_fffb70c0c</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.3260</td>\n",
       "      <td>0.3478</td>\n",
       "      <td>-0.3743</td>\n",
       "      <td>0.9905</td>\n",
       "      <td>-0.7178</td>\n",
       "      <td>0.6621</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4286</td>\n",
       "      <td>0.4426</td>\n",
       "      <td>0.0423</td>\n",
       "      <td>-0.3195</td>\n",
       "      <td>-0.8086</td>\n",
       "      <td>-0.9798</td>\n",
       "      <td>-0.2084</td>\n",
       "      <td>-0.1224</td>\n",
       "      <td>-0.2715</td>\n",
       "      <td>0.3689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>id_fffc1c3f4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3942</td>\n",
       "      <td>0.3756</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>-0.7389</td>\n",
       "      <td>0.5505</td>\n",
       "      <td>-0.0159</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5409</td>\n",
       "      <td>0.3755</td>\n",
       "      <td>0.7343</td>\n",
       "      <td>0.2807</td>\n",
       "      <td>0.4116</td>\n",
       "      <td>0.6422</td>\n",
       "      <td>0.2256</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.3808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>id_fffcb9e7c</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.6660</td>\n",
       "      <td>0.2324</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.2044</td>\n",
       "      <td>0.8531</td>\n",
       "      <td>-0.0343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1105</td>\n",
       "      <td>0.4258</td>\n",
       "      <td>-0.2012</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>1.5230</td>\n",
       "      <td>0.7101</td>\n",
       "      <td>0.1732</td>\n",
       "      <td>0.7015</td>\n",
       "      <td>-0.6290</td>\n",
       "      <td>0.0740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>id_ffffdd77b</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.8598</td>\n",
       "      <td>1.0240</td>\n",
       "      <td>-0.1361</td>\n",
       "      <td>0.7952</td>\n",
       "      <td>-0.3611</td>\n",
       "      <td>-3.6750</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.3890</td>\n",
       "      <td>-1.7450</td>\n",
       "      <td>-6.6300</td>\n",
       "      <td>-4.0950</td>\n",
       "      <td>-7.3860</td>\n",
       "      <td>-1.4160</td>\n",
       "      <td>-3.5770</td>\n",
       "      <td>-0.4775</td>\n",
       "      <td>-2.1500</td>\n",
       "      <td>-4.2520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 876 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             sig_id  cp_type  cp_time  cp_dose     g-0     g-1     g-2  \\\n",
       "0      id_000644bb2        0        1        0  1.0620  0.5577 -0.2479   \n",
       "1      id_000779bfc        0        3        0  0.0743  0.4087  0.2991   \n",
       "2      id_000a6266a        0        2        0  0.6280  0.5817  1.5540   \n",
       "3      id_0015fd391        0        2        0 -0.5138 -0.2491 -0.2656   \n",
       "4      id_001626bd3        0        3        1 -0.3254 -0.4009  0.9700   \n",
       "...             ...      ...      ...      ...     ...     ...     ...   \n",
       "23809  id_fffb1ceed        0        1        1  0.1394 -0.0636 -0.1112   \n",
       "23810  id_fffb70c0c        0        1        1 -1.3260  0.3478 -0.3743   \n",
       "23811  id_fffc1c3f4        1        2        1  0.3942  0.3756  0.3109   \n",
       "23812  id_fffcb9e7c        0        1        0  0.6660  0.2324  0.4392   \n",
       "23813  id_ffffdd77b        0        3        0 -0.8598  1.0240 -0.1361   \n",
       "\n",
       "          g-3     g-4     g-5  ...    c-90    c-91    c-92    c-93    c-94  \\\n",
       "0     -0.6208 -0.1944 -1.0120  ...  0.2862  0.2584  0.8076  0.5523 -0.1912   \n",
       "1      0.0604  1.0190  0.5207  ... -0.4265  0.7543  0.4708  0.0230  0.2957   \n",
       "2     -0.0764 -0.0323  1.2390  ... -0.7250 -0.6297  0.6103  0.0223 -1.3240   \n",
       "3      0.5288  4.0620 -0.8095  ... -2.0990 -0.6441 -5.6300 -1.3780 -0.8632   \n",
       "4      0.6919  1.4180 -0.8244  ...  0.0042  0.0048  0.6670  1.0690  0.5523   \n",
       "...       ...     ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "23809 -0.5080 -0.4713  0.7201  ...  0.1969  0.0262 -0.8121  0.3434  0.5372   \n",
       "23810  0.9905 -0.7178  0.6621  ...  0.4286  0.4426  0.0423 -0.3195 -0.8086   \n",
       "23811 -0.7389  0.5505 -0.0159  ...  0.5409  0.3755  0.7343  0.2807  0.4116   \n",
       "23812  0.2044  0.8531 -0.0343  ... -0.1105  0.4258 -0.2012  0.1506  1.5230   \n",
       "23813  0.7952 -0.3611 -3.6750  ... -3.3890 -1.7450 -6.6300 -4.0950 -7.3860   \n",
       "\n",
       "         c-95    c-96    c-97    c-98    c-99  \n",
       "0      0.6584 -0.3981  0.2139  0.3801  0.4176  \n",
       "1      0.4899  0.1522  0.1241  0.6077  0.7371  \n",
       "2     -0.3174 -0.6417 -0.2187 -1.4080  0.6931  \n",
       "3     -1.2880 -1.6210 -0.8784 -0.3876 -0.8154  \n",
       "4     -0.3031  0.1094  0.2885 -0.3786  0.7125  \n",
       "...       ...     ...     ...     ...     ...  \n",
       "23809 -0.3246  0.0631  0.9171  0.5258  0.4680  \n",
       "23810 -0.9798 -0.2084 -0.1224 -0.2715  0.3689  \n",
       "23811  0.6422  0.2256  0.7592  0.6656  0.3808  \n",
       "23812  0.7101  0.1732  0.7015 -0.6290  0.0740  \n",
       "23813 -1.4160 -3.5770 -0.4775 -2.1500 -4.2520  \n",
       "\n",
       "[23814 rows x 876 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    df['cp_type'] = df['cp_type'].map({'trt_cp':0, 'ctl_vehicle':1})\n",
    "    df['cp_time'] = df['cp_time'].map({24:1, 48:2, 72:3})\n",
    "    df['cp_dose'] = df['cp_dose'].map({'D1':0, 'D2':1})\n",
    "    return df\n",
    "X_train = preprocess(df_train)\n",
    "X_test = preprocess(df_test)\n",
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\"\"\"\n",
    "PCA on Genes \n",
    "\"\"\"\n",
    "n_comp = 200\n",
    "\n",
    "genes = [col for col in X_train.columns if col.startswith('g-')]\n",
    "cells = [col for col in X_train.columns if col.startswith('c-')]\n",
    "\n",
    "data_genes = pd.concat([pd.DataFrame(X_train[genes]), pd.DataFrame(X_test[genes])])\n",
    "data_genes_pca = PCA(n_components=n_comp, random_state=42).fit_transform(data_genes)\n",
    "\n",
    "train_gene_pca = data_genes_pca[:X_train.shape[0]]\n",
    "test_gene_pca = data_genes_pca[-X_test.shape[0]:]\n",
    "\n",
    "train_gene_pca = pd.DataFrame(train_gene_pca, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test_gene_pca = pd.DataFrame(test_gene_pca, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "\"\"\"\n",
    "PCA on Cells\n",
    "\"\"\"\n",
    "n_comp = 50 \n",
    "\n",
    "data_cells = pd.concat([pd.DataFrame(X_train[cells]), pd.DataFrame(X_test[cells])])\n",
    "data_cells_pca = PCA(n_components=n_comp, random_state=42).fit_transform(data_cells)\n",
    "\n",
    "train_cells_pca = data_cells_pca[:X_train.shape[0]] \n",
    "test_cells_pca = data_cells_pca[-X_test.shape[0]:]\n",
    "\n",
    "train_cells_pca = pd.DataFrame(train_cells_pca, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test_cells_pca = pd.DataFrame(test_cells_pca, columns=[f'pca_C-{i}' for i in range(n_comp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_G-0</th>\n",
       "      <th>pca_G-1</th>\n",
       "      <th>pca_G-2</th>\n",
       "      <th>pca_G-3</th>\n",
       "      <th>pca_G-4</th>\n",
       "      <th>pca_G-5</th>\n",
       "      <th>pca_G-6</th>\n",
       "      <th>pca_G-7</th>\n",
       "      <th>pca_G-8</th>\n",
       "      <th>pca_G-9</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_C-40</th>\n",
       "      <th>pca_C-41</th>\n",
       "      <th>pca_C-42</th>\n",
       "      <th>pca_C-43</th>\n",
       "      <th>pca_C-44</th>\n",
       "      <th>pca_C-45</th>\n",
       "      <th>pca_C-46</th>\n",
       "      <th>pca_C-47</th>\n",
       "      <th>pca_C-48</th>\n",
       "      <th>pca_C-49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.191034</td>\n",
       "      <td>-0.488171</td>\n",
       "      <td>-3.967531</td>\n",
       "      <td>6.922384</td>\n",
       "      <td>3.426204</td>\n",
       "      <td>-4.939660</td>\n",
       "      <td>-4.021629</td>\n",
       "      <td>3.108678</td>\n",
       "      <td>2.719793</td>\n",
       "      <td>-2.409787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511125</td>\n",
       "      <td>1.022494</td>\n",
       "      <td>-0.136036</td>\n",
       "      <td>0.075919</td>\n",
       "      <td>-0.066207</td>\n",
       "      <td>-0.101230</td>\n",
       "      <td>0.225362</td>\n",
       "      <td>0.057230</td>\n",
       "      <td>-0.107296</td>\n",
       "      <td>-0.341431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.531898</td>\n",
       "      <td>3.288575</td>\n",
       "      <td>9.420453</td>\n",
       "      <td>-0.574416</td>\n",
       "      <td>-2.009119</td>\n",
       "      <td>4.779334</td>\n",
       "      <td>2.585964</td>\n",
       "      <td>1.996409</td>\n",
       "      <td>0.305785</td>\n",
       "      <td>1.425774</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.319058</td>\n",
       "      <td>0.319740</td>\n",
       "      <td>-0.159246</td>\n",
       "      <td>-0.345882</td>\n",
       "      <td>0.150300</td>\n",
       "      <td>-0.216133</td>\n",
       "      <td>0.687300</td>\n",
       "      <td>0.400472</td>\n",
       "      <td>-0.136589</td>\n",
       "      <td>0.428745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.669338</td>\n",
       "      <td>2.297153</td>\n",
       "      <td>-0.782439</td>\n",
       "      <td>-7.036332</td>\n",
       "      <td>-1.434308</td>\n",
       "      <td>-1.718394</td>\n",
       "      <td>2.841784</td>\n",
       "      <td>-0.774921</td>\n",
       "      <td>-1.731597</td>\n",
       "      <td>-4.942427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149934</td>\n",
       "      <td>-0.867463</td>\n",
       "      <td>-0.574558</td>\n",
       "      <td>-0.320742</td>\n",
       "      <td>0.147007</td>\n",
       "      <td>0.119810</td>\n",
       "      <td>0.768706</td>\n",
       "      <td>-1.156860</td>\n",
       "      <td>-0.365257</td>\n",
       "      <td>-0.456086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.268336</td>\n",
       "      <td>-4.829223</td>\n",
       "      <td>-1.752497</td>\n",
       "      <td>-11.326041</td>\n",
       "      <td>-1.178113</td>\n",
       "      <td>-10.384511</td>\n",
       "      <td>5.975081</td>\n",
       "      <td>-2.337151</td>\n",
       "      <td>-0.157038</td>\n",
       "      <td>4.973514</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.084507</td>\n",
       "      <td>0.665303</td>\n",
       "      <td>-0.475641</td>\n",
       "      <td>-0.746390</td>\n",
       "      <td>1.684805</td>\n",
       "      <td>-0.698474</td>\n",
       "      <td>-0.320341</td>\n",
       "      <td>0.882071</td>\n",
       "      <td>1.369027</td>\n",
       "      <td>1.143375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-7.185918</td>\n",
       "      <td>0.111345</td>\n",
       "      <td>8.256769</td>\n",
       "      <td>-7.394481</td>\n",
       "      <td>-0.632286</td>\n",
       "      <td>-4.333923</td>\n",
       "      <td>-1.741193</td>\n",
       "      <td>0.646718</td>\n",
       "      <td>-6.767281</td>\n",
       "      <td>5.830797</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.127680</td>\n",
       "      <td>-0.199497</td>\n",
       "      <td>0.697061</td>\n",
       "      <td>-0.557235</td>\n",
       "      <td>-0.913216</td>\n",
       "      <td>0.174704</td>\n",
       "      <td>0.175920</td>\n",
       "      <td>0.543692</td>\n",
       "      <td>0.291145</td>\n",
       "      <td>0.513161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23809</th>\n",
       "      <td>-6.018252</td>\n",
       "      <td>2.927708</td>\n",
       "      <td>-0.626853</td>\n",
       "      <td>-3.892167</td>\n",
       "      <td>-0.773846</td>\n",
       "      <td>-0.904330</td>\n",
       "      <td>1.530116</td>\n",
       "      <td>0.114901</td>\n",
       "      <td>2.579791</td>\n",
       "      <td>-1.943151</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.287854</td>\n",
       "      <td>-0.227900</td>\n",
       "      <td>0.129922</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>0.659682</td>\n",
       "      <td>-0.359301</td>\n",
       "      <td>0.600943</td>\n",
       "      <td>-0.788161</td>\n",
       "      <td>-0.156615</td>\n",
       "      <td>-1.434734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23810</th>\n",
       "      <td>-4.952320</td>\n",
       "      <td>-0.545998</td>\n",
       "      <td>-1.626794</td>\n",
       "      <td>0.221293</td>\n",
       "      <td>4.770532</td>\n",
       "      <td>-0.212782</td>\n",
       "      <td>0.308716</td>\n",
       "      <td>-2.716075</td>\n",
       "      <td>-1.425005</td>\n",
       "      <td>0.317466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101267</td>\n",
       "      <td>-0.316646</td>\n",
       "      <td>-0.221708</td>\n",
       "      <td>-0.471236</td>\n",
       "      <td>-0.263232</td>\n",
       "      <td>-0.692952</td>\n",
       "      <td>-1.059063</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.985178</td>\n",
       "      <td>0.260118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23811</th>\n",
       "      <td>-6.522456</td>\n",
       "      <td>-1.372804</td>\n",
       "      <td>-0.831251</td>\n",
       "      <td>0.531181</td>\n",
       "      <td>-0.837211</td>\n",
       "      <td>-1.502036</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>2.996020</td>\n",
       "      <td>-1.024627</td>\n",
       "      <td>-0.891801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542585</td>\n",
       "      <td>0.421446</td>\n",
       "      <td>0.047425</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>-0.890236</td>\n",
       "      <td>0.125915</td>\n",
       "      <td>-0.517586</td>\n",
       "      <td>0.500146</td>\n",
       "      <td>0.305829</td>\n",
       "      <td>-0.503617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23812</th>\n",
       "      <td>7.535654</td>\n",
       "      <td>-20.646006</td>\n",
       "      <td>-1.614500</td>\n",
       "      <td>12.610355</td>\n",
       "      <td>-8.006033</td>\n",
       "      <td>-0.190805</td>\n",
       "      <td>-3.018044</td>\n",
       "      <td>5.666977</td>\n",
       "      <td>7.751807</td>\n",
       "      <td>7.323825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472907</td>\n",
       "      <td>-1.584457</td>\n",
       "      <td>-0.692551</td>\n",
       "      <td>0.229086</td>\n",
       "      <td>-0.850729</td>\n",
       "      <td>0.451430</td>\n",
       "      <td>0.720103</td>\n",
       "      <td>0.530429</td>\n",
       "      <td>0.165919</td>\n",
       "      <td>0.857855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23813</th>\n",
       "      <td>2.832385</td>\n",
       "      <td>4.535160</td>\n",
       "      <td>-6.824516</td>\n",
       "      <td>5.757672</td>\n",
       "      <td>5.523924</td>\n",
       "      <td>-18.432074</td>\n",
       "      <td>9.000305</td>\n",
       "      <td>-6.994868</td>\n",
       "      <td>7.222085</td>\n",
       "      <td>-0.504322</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117023</td>\n",
       "      <td>-0.444889</td>\n",
       "      <td>-1.842296</td>\n",
       "      <td>0.972261</td>\n",
       "      <td>-1.760630</td>\n",
       "      <td>1.072521</td>\n",
       "      <td>0.743064</td>\n",
       "      <td>0.638026</td>\n",
       "      <td>0.285524</td>\n",
       "      <td>0.715814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23814 rows × 250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        pca_G-0    pca_G-1   pca_G-2    pca_G-3   pca_G-4    pca_G-5  \\\n",
       "0     -8.191034  -0.488171 -3.967531   6.922384  3.426204  -4.939660   \n",
       "1     -6.531898   3.288575  9.420453  -0.574416 -2.009119   4.779334   \n",
       "2     -1.669338   2.297153 -0.782439  -7.036332 -1.434308  -1.718394   \n",
       "3      9.268336  -4.829223 -1.752497 -11.326041 -1.178113 -10.384511   \n",
       "4     -7.185918   0.111345  8.256769  -7.394481 -0.632286  -4.333923   \n",
       "...         ...        ...       ...        ...       ...        ...   \n",
       "23809 -6.018252   2.927708 -0.626853  -3.892167 -0.773846  -0.904330   \n",
       "23810 -4.952320  -0.545998 -1.626794   0.221293  4.770532  -0.212782   \n",
       "23811 -6.522456  -1.372804 -0.831251   0.531181 -0.837211  -1.502036   \n",
       "23812  7.535654 -20.646006 -1.614500  12.610355 -8.006033  -0.190805   \n",
       "23813  2.832385   4.535160 -6.824516   5.757672  5.523924 -18.432074   \n",
       "\n",
       "        pca_G-6   pca_G-7   pca_G-8   pca_G-9  ...  pca_C-40  pca_C-41  \\\n",
       "0     -4.021629  3.108678  2.719793 -2.409787  ... -0.511125  1.022494   \n",
       "1      2.585964  1.996409  0.305785  1.425774  ... -0.319058  0.319740   \n",
       "2      2.841784 -0.774921 -1.731597 -4.942427  ... -0.149934 -0.867463   \n",
       "3      5.975081 -2.337151 -0.157038  4.973514  ... -1.084507  0.665303   \n",
       "4     -1.741193  0.646718 -6.767281  5.830797  ... -1.127680 -0.199497   \n",
       "...         ...       ...       ...       ...  ...       ...       ...   \n",
       "23809  1.530116  0.114901  2.579791 -1.943151  ... -0.287854 -0.227900   \n",
       "23810  0.308716 -2.716075 -1.425005  0.317466  ... -0.101267 -0.316646   \n",
       "23811  0.003087  2.996020 -1.024627 -0.891801  ... -0.542585  0.421446   \n",
       "23812 -3.018044  5.666977  7.751807  7.323825  ...  0.472907 -1.584457   \n",
       "23813  9.000305 -6.994868  7.222085 -0.504322  ... -1.117023 -0.444889   \n",
       "\n",
       "       pca_C-42  pca_C-43  pca_C-44  pca_C-45  pca_C-46  pca_C-47  pca_C-48  \\\n",
       "0     -0.136036  0.075919 -0.066207 -0.101230  0.225362  0.057230 -0.107296   \n",
       "1     -0.159246 -0.345882  0.150300 -0.216133  0.687300  0.400472 -0.136589   \n",
       "2     -0.574558 -0.320742  0.147007  0.119810  0.768706 -1.156860 -0.365257   \n",
       "3     -0.475641 -0.746390  1.684805 -0.698474 -0.320341  0.882071  1.369027   \n",
       "4      0.697061 -0.557235 -0.913216  0.174704  0.175920  0.543692  0.291145   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "23809  0.129922  0.478464  0.659682 -0.359301  0.600943 -0.788161 -0.156615   \n",
       "23810 -0.221708 -0.471236 -0.263232 -0.692952 -1.059063  0.699861  0.985178   \n",
       "23811  0.047425 -0.000176 -0.890236  0.125915 -0.517586  0.500146  0.305829   \n",
       "23812 -0.692551  0.229086 -0.850729  0.451430  0.720103  0.530429  0.165919   \n",
       "23813 -1.842296  0.972261 -1.760630  1.072521  0.743064  0.638026  0.285524   \n",
       "\n",
       "       pca_C-49  \n",
       "0     -0.341431  \n",
       "1      0.428745  \n",
       "2     -0.456086  \n",
       "3      1.143375  \n",
       "4      0.513161  \n",
       "...         ...  \n",
       "23809 -1.434734  \n",
       "23810  0.260118  \n",
       "23811 -0.503617  \n",
       "23812  0.857855  \n",
       "23813  0.715814  \n",
       "\n",
       "[23814 rows x 250 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_features = pd.concat((train_gene_pca, train_cells_pca), axis=1)\n",
    "test_features = pd.concat((test_gene_pca, test_cells_pca), axis=1)\n",
    "\n",
    "# y_labels\n",
    "df_target_s = df_target_s.drop(['sig_id'], axis=1)\n",
    "\n",
    "display(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 7938  7939  7940 ... 23811 23812 23813] TEST: [   0    1    2 ... 7935 7936 7937]\n",
      "loss:  5.038902350517758\n",
      "TRAIN: [    0     1     2 ... 23811 23812 23813] TEST: [ 7938  7939  7940 ... 15873 15874 15875]\n",
      "loss:  5.138453919425258\n",
      "TRAIN: [    0     1     2 ... 15873 15874 15875] TEST: [15876 15877 15878 ... 23811 23812 23813]\n",
      "loss:  5.124433833132402\n",
      "average log loss:  6.780230817864393\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "\n",
    "\"\"\"\n",
    "Linear Model - Logistic Regression (multinomial)\n",
    "\"\"\"\n",
    "# 3_Fold cross validation\n",
    "n_splits = 3\n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "kf.get_n_splits(train_features)\n",
    "\n",
    "log_Loss = 0.0\n",
    "\n",
    "for train_index, test_index in kf.split(train_features):\n",
    "    \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_, X_val_ = train_features.iloc[train_index], train_features.iloc[test_index]\n",
    "    y_train_, y_val_ = df_target_s.iloc[train_index], df_target_s.iloc[test_index]\n",
    "    \n",
    "    X_train_, X_val_ = np.array(X_train_), np.array(X_val_)\n",
    "    y_train_, y_val_ = np.array(y_train_), np.array(y_val_)\n",
    "    \n",
    "    # fix for empty columns - LogisticRegression blows up otherwise \n",
    "    check_for_empty_cols = np.where(y_train_.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "        y_train_[0, check_for_empty_cols] = 1\n",
    "    \n",
    "    clf = MultiOutputClassifier(LogisticRegression(max_iter=10000, tol=0.1, C = 0.5), n_jobs=-1)\n",
    "    clf.fit(X_train_, y_train_)\n",
    "    y_pred = clf.predict(X_val_)\n",
    "    \n",
    "    # calc averag log loss\n",
    "    print(\"loss: \", log_loss(y_val_, y_pred))\n",
    "    loss += log_loss(y_val_, y_pred)\n",
    "    \n",
    "    \n",
    "loss /= n_splits\n",
    "print(\"average log loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7938, 206)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 206 is out of bounds for axis 1 with size 206",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-82a7fb381db8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlog_Loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0my_pred_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0my_val_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_val_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# fix empty y_val columns if there is any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 206 is out of bounds for axis 1 with size 206"
     ]
    }
   ],
   "source": [
    "log_Loss = 0.0\n",
    "for c in range(y_pred.shape[0]):\n",
    "    y_pred_col = y_pred[:, c]\n",
    "    y_val_col = y_val_[:, c]\n",
    "    # fix empty y_val columns if there is any\n",
    "    if y_val_col.sum()==0:\n",
    "        y_val_col[0] = 1\n",
    "    log_Loss += log_loss(y_val_col, y_pred_col)\n",
    "    \n",
    "print(log_Loss)\n",
    "    \n",
    "\n",
    "print(log_Loss / y_pred.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   1,   2,   1,   4,   6,  13,   4,   2,   8,   0,  19,   2,\n",
       "        11,   1,   0,  12,   1,   6,   9,   5,  13,   0,  14,   0,   0,\n",
       "         2,   3,  11,   8,   0,  17,   7,   0,   0,   1,  17,   2,  32,\n",
       "         0,   6,   9,   9,   6,   6,   4,   1,   4,   7,  10,   1,  19,\n",
       "         2,   0,   2,  12,  13,  12,   7,   2,   0,   4,   2,  99,   5,\n",
       "         4,  10,  21,  15,   0,   4,   0,   5,   9,  10,   0,  10,  14,\n",
       "         3,   5, 110,   0,   0,   8,  16,   8,   1,   6,  18,  79,   3,\n",
       "         0,   2,   5,   3,  11,  86,   0,   2,   1,   1,  18,   4,  30,\n",
       "         9,   2,   1,   1,  13,  75,  32,   5,   5,   8,  21,   9,  12,\n",
       "         8,  30,  94,   0,   1,   8,   0,  13,   0,   8,  18,  12,   8,\n",
       "         0,   3,   2,  41,   8,   8, 259,   0,   1,   1,   1,   0,   0,\n",
       "         7,   8,   8,  16,   2,  22,  96,   1,   1,   4,  32,   1,  14,\n",
       "         2,  16,   7,  16,   3,   7,   4, 254,  12,   0,  12,   0,  10,\n",
       "        78,   0,  12,   0,   2,  13,   5,   2,   1,   8,   9,  11,   2,\n",
       "         0,   4,  20,   0,   4,  14,   4,   1,   9,   4,   0,   5,  53,\n",
       "         0,   1,   2,  13,  96,  14,   0,  47,   3,   4,   5], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 7938  7939  7940 ... 23811 23812 23813] TEST: [   0    1    2 ... 7935 7936 7937]\n",
      "loss:  3.3897107329389637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 3_Fold cross validation\n",
    "n_splits = 3\n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "kf.get_n_splits(train_features)\n",
    "\n",
    "log_Loss = 0.0\n",
    "\n",
    "for train_index, test_index in kf.split(train_features):\n",
    "    \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_, X_val_ = train_features.iloc[train_index], train_features.iloc[test_index]\n",
    "    y_train_, y_val_ = df_target_s.iloc[train_index], df_target_s.iloc[test_index]\n",
    "    \n",
    "    X_train_, X_val_ = np.array(X_train_), np.array(X_val_)\n",
    "    y_train_, y_val_ = np.array(y_train_), np.array(y_val_)\n",
    "    \n",
    "    # fix for empty columns - LogisticRegression blows up otherwise \n",
    "    check_for_empty_cols = np.where(y_train_.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "        y_train_[0, check_for_empty_cols] = 1\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=200,max_depth=10, random_state=43,min_samples_split=10)\n",
    "    clf.fit(X_train_, y_train_)\n",
    "    y_pred = clf.predict(X_val_)\n",
    "    \n",
    "    # calc averag log loss\n",
    "    loss += calc_log_loss(y_val_, y_pred)\n",
    "    print(\"loss: \", loss)\n",
    "    break\n",
    "    \n",
    "    \n",
    "# loss /= n_splits\n",
    "# print(\"average log loss: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  59,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,  15,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   8,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,  15,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0, 236,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0, 236,   0,   0,   0,   0,   0,\n",
       "        43,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,  30,   0,   0,   0,   0,   0,   0], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_log_loss(y_true, y_pred):\n",
    "    log_Loss = 0.0\n",
    "    for c in range(y_pred.shape[1]):\n",
    "        y_pred_col = y_pred[:, c]\n",
    "        y_val_col = y_true[:, c]\n",
    "        # fix empty y_val columns if there is any\n",
    "        if y_val_col.sum()==0:\n",
    "            y_val_col[0] = 1\n",
    "        log_Loss += log_loss(y_val_col, y_pred_col)\n",
    "    return (log_Loss / y_pred.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 7938  7939  7940 ... 23811 23812 23813] TEST: [   0    1    2 ... 7935 7936 7937]\n",
      "Iteration 1, loss = 42.99590457\n",
      "Iteration 2, loss = 5.54297738\n",
      "Iteration 3, loss = 4.48677032\n",
      "Iteration 4, loss = 4.14306158\n",
      "Iteration 5, loss = 3.94652223\n",
      "Iteration 6, loss = 3.80286040\n",
      "Iteration 7, loss = 3.68554338\n",
      "Iteration 8, loss = 3.58586791\n",
      "Iteration 9, loss = 3.49817901\n",
      "Iteration 10, loss = 3.41959404\n",
      "Iteration 11, loss = 3.34823711\n",
      "Iteration 12, loss = 3.28317527\n",
      "Iteration 13, loss = 3.22223705\n",
      "Iteration 14, loss = 3.16426173\n",
      "Iteration 15, loss = 3.10923112\n",
      "Iteration 16, loss = 3.05739863\n",
      "Iteration 17, loss = 3.00756836\n",
      "Iteration 18, loss = 2.96075274\n",
      "Iteration 19, loss = 2.91452584\n",
      "Iteration 20, loss = 2.87037213\n",
      "Iteration 21, loss = 2.82754729\n",
      "Iteration 22, loss = 2.78580015\n",
      "Iteration 23, loss = 2.74530670\n",
      "Iteration 24, loss = 2.70658176\n",
      "Iteration 25, loss = 2.66799632\n",
      "Iteration 26, loss = 2.63039656\n",
      "Iteration 27, loss = 2.59435412\n",
      "Iteration 28, loss = 2.55809771\n",
      "Iteration 29, loss = 2.52195841\n",
      "Iteration 30, loss = 2.48724558\n",
      "Iteration 31, loss = 2.45273747\n",
      "Iteration 32, loss = 2.42084820\n",
      "Iteration 33, loss = 2.38687186\n",
      "Iteration 34, loss = 2.35422620\n",
      "Iteration 35, loss = 2.32360573\n",
      "Iteration 36, loss = 2.29188173\n",
      "Iteration 37, loss = 2.26124508\n",
      "Iteration 38, loss = 2.23087271\n",
      "Iteration 39, loss = 2.20018980\n",
      "Iteration 40, loss = 2.17139863\n",
      "Iteration 41, loss = 2.14314603\n",
      "Iteration 42, loss = 2.11504251\n",
      "Iteration 43, loss = 2.08477090\n",
      "Iteration 44, loss = 2.05875553\n",
      "Iteration 45, loss = 2.03209177\n",
      "Iteration 46, loss = 2.00548029\n",
      "Iteration 47, loss = 1.97890602\n",
      "Iteration 48, loss = 1.95371292\n",
      "Iteration 49, loss = 1.92871784\n",
      "Iteration 50, loss = 1.90508455\n",
      "Iteration 51, loss = 1.87873596\n",
      "Iteration 52, loss = 1.85378371\n",
      "Iteration 53, loss = 1.83178642\n",
      "Iteration 54, loss = 1.80802216\n",
      "Iteration 55, loss = 1.78614678\n",
      "Iteration 56, loss = 1.76502564\n",
      "Iteration 57, loss = 1.74295938\n",
      "Iteration 58, loss = 1.72061622\n",
      "Iteration 59, loss = 1.70030348\n",
      "Iteration 60, loss = 1.67896643\n",
      "Iteration 61, loss = 1.65958320\n",
      "Iteration 62, loss = 1.63951952\n",
      "Iteration 63, loss = 1.61975452\n",
      "Iteration 64, loss = 1.60088447\n",
      "Iteration 65, loss = 1.58032022\n",
      "Iteration 66, loss = 1.56247531\n",
      "Iteration 67, loss = 1.54239617\n",
      "Iteration 68, loss = 1.52544967\n",
      "Iteration 69, loss = 1.50864822\n",
      "Iteration 70, loss = 1.49133396\n",
      "Iteration 71, loss = 1.47436931\n",
      "Iteration 72, loss = 1.45784551\n",
      "Iteration 73, loss = 1.43994782\n",
      "Iteration 74, loss = 1.42406873\n",
      "Iteration 75, loss = 1.40844587\n",
      "Iteration 76, loss = 1.39549360\n",
      "Iteration 77, loss = 1.37876141\n",
      "Iteration 78, loss = 1.36369803\n",
      "Iteration 79, loss = 1.34851828\n",
      "Iteration 80, loss = 1.33407804\n",
      "Iteration 81, loss = 1.31974235\n",
      "Iteration 82, loss = 1.30675838\n",
      "Iteration 83, loss = 1.29209710\n",
      "Iteration 84, loss = 1.27931311\n",
      "Iteration 85, loss = 1.26471785\n",
      "Iteration 86, loss = 1.25270029\n",
      "Iteration 87, loss = 1.23942016\n",
      "Iteration 88, loss = 1.22652176\n",
      "Iteration 89, loss = 1.21690646\n",
      "Iteration 90, loss = 1.20376751\n",
      "Iteration 91, loss = 1.19073507\n",
      "Iteration 92, loss = 1.17849430\n",
      "Iteration 93, loss = 1.16770236\n",
      "Iteration 94, loss = 1.15424888\n",
      "Iteration 95, loss = 1.14503525\n",
      "Iteration 96, loss = 1.13488008\n",
      "Iteration 97, loss = 1.12392543\n",
      "Iteration 98, loss = 1.11168559\n",
      "Iteration 99, loss = 1.10142033\n",
      "Iteration 100, loss = 1.08890564\n",
      "Iteration 101, loss = 1.08205770\n",
      "Iteration 102, loss = 1.07111420\n",
      "Iteration 103, loss = 1.06087180\n",
      "Iteration 104, loss = 1.05032272\n",
      "Iteration 105, loss = 1.04132392\n",
      "Iteration 106, loss = 1.03104447\n",
      "Iteration 107, loss = 1.02139843\n",
      "Iteration 108, loss = 1.01108691\n",
      "Iteration 109, loss = 1.00182820\n",
      "Iteration 110, loss = 0.99415665\n",
      "Iteration 111, loss = 0.98476268\n",
      "Iteration 112, loss = 0.97425338\n",
      "Iteration 113, loss = 0.96648560\n",
      "Iteration 114, loss = 0.95799073\n",
      "Iteration 115, loss = 0.95021587\n",
      "Iteration 116, loss = 0.94398792\n",
      "Iteration 117, loss = 0.93316459\n",
      "Iteration 118, loss = 0.92394423\n",
      "Iteration 119, loss = 0.91609112\n",
      "Iteration 120, loss = 0.90783093\n",
      "Iteration 121, loss = 0.90232896\n",
      "Iteration 122, loss = 0.89353856\n",
      "Iteration 123, loss = 0.88392892\n",
      "Iteration 124, loss = 0.87718007\n",
      "Iteration 125, loss = 0.86812873\n",
      "Iteration 126, loss = 0.85997039\n",
      "Iteration 127, loss = 0.85363024\n",
      "Iteration 128, loss = 0.84792317\n",
      "Iteration 129, loss = 0.83864445\n",
      "Iteration 130, loss = 0.83212793\n",
      "Iteration 131, loss = 0.82614937\n",
      "Iteration 132, loss = 0.81661025\n",
      "Iteration 133, loss = 0.81006667\n",
      "Iteration 134, loss = 0.80332617\n",
      "Iteration 135, loss = 0.79617251\n",
      "Iteration 136, loss = 0.79002389\n",
      "Iteration 137, loss = 0.78498730\n",
      "Iteration 138, loss = 0.77798495\n",
      "Iteration 139, loss = 0.77198017\n",
      "Iteration 140, loss = 0.76212251\n",
      "Iteration 141, loss = 0.75712287\n",
      "Iteration 142, loss = 0.75129920\n",
      "Iteration 143, loss = 0.74471267\n",
      "Iteration 144, loss = 0.73871750\n",
      "Iteration 145, loss = 0.73118667\n",
      "Iteration 146, loss = 0.72632459\n",
      "Iteration 147, loss = 0.71898978\n",
      "Iteration 148, loss = 0.71172681\n",
      "Iteration 149, loss = 0.70628283\n",
      "Iteration 150, loss = 0.70102304\n",
      "Iteration 151, loss = 0.69617146\n",
      "Iteration 152, loss = 0.68924831\n",
      "Iteration 153, loss = 0.68345685\n",
      "Iteration 154, loss = 0.67641494\n",
      "Iteration 155, loss = 0.67368710\n",
      "Iteration 156, loss = 0.66711172\n",
      "Iteration 157, loss = 0.66195136\n",
      "Iteration 158, loss = 0.65548865\n",
      "Iteration 159, loss = 0.65012137\n",
      "Iteration 160, loss = 0.64427429\n",
      "Iteration 161, loss = 0.63792327\n",
      "Iteration 162, loss = 0.63438580\n",
      "Iteration 163, loss = 0.62865975\n",
      "Iteration 164, loss = 0.62359375\n",
      "Iteration 165, loss = 0.61794059\n",
      "Iteration 166, loss = 0.61300779\n",
      "Iteration 167, loss = 0.60743078\n",
      "Iteration 168, loss = 0.60181459\n",
      "Iteration 169, loss = 0.59813274\n",
      "Iteration 170, loss = 0.59503951\n",
      "Iteration 171, loss = 0.59017059\n",
      "Iteration 172, loss = 0.58419129\n",
      "Iteration 173, loss = 0.57818427\n",
      "Iteration 174, loss = 0.57470192\n",
      "Iteration 175, loss = 0.57059366\n",
      "Iteration 176, loss = 0.56445101\n",
      "Iteration 177, loss = 0.55946882\n",
      "Iteration 178, loss = 0.55450736\n",
      "Iteration 179, loss = 0.54914107\n",
      "Iteration 180, loss = 0.54637684\n",
      "Iteration 181, loss = 0.54268126\n",
      "Iteration 182, loss = 0.53563926\n",
      "Iteration 183, loss = 0.53193047\n",
      "Iteration 184, loss = 0.52621480\n",
      "Iteration 185, loss = 0.52350707\n",
      "Iteration 186, loss = 0.51794234\n",
      "Iteration 187, loss = 0.51456127\n",
      "Iteration 188, loss = 0.50889689\n",
      "Iteration 189, loss = 0.50539303\n",
      "Iteration 190, loss = 0.49995946\n",
      "Iteration 191, loss = 0.50029078\n",
      "Iteration 192, loss = 0.49323794\n",
      "Iteration 193, loss = 0.48971025\n",
      "Iteration 194, loss = 0.48469775\n",
      "Iteration 195, loss = 0.48029940\n",
      "Iteration 196, loss = 0.47705941\n",
      "Iteration 197, loss = 0.47219391\n",
      "Iteration 198, loss = 0.46726590\n",
      "Iteration 199, loss = 0.46489776\n",
      "Iteration 200, loss = 0.45975199\n",
      "Iteration 201, loss = 0.45600832\n",
      "Iteration 202, loss = 0.45171222\n",
      "Iteration 203, loss = 0.44754564\n",
      "Iteration 204, loss = 0.44538868\n",
      "Iteration 205, loss = 0.44264560\n",
      "Iteration 206, loss = 0.43695840\n",
      "Iteration 207, loss = 0.43109738\n",
      "Iteration 208, loss = 0.42803857\n",
      "Iteration 209, loss = 0.42513695\n",
      "Iteration 210, loss = 0.42178812\n",
      "Iteration 211, loss = 0.42112002\n",
      "Iteration 212, loss = 0.42204025\n",
      "Iteration 213, loss = 0.41416327\n",
      "Iteration 214, loss = 0.40853003\n",
      "Iteration 215, loss = 0.40426344\n",
      "Iteration 216, loss = 0.39784536\n",
      "Iteration 217, loss = 0.39430551\n",
      "Iteration 218, loss = 0.39232534\n",
      "Iteration 219, loss = 0.38750130\n",
      "Iteration 220, loss = 0.38279504\n",
      "Iteration 221, loss = 0.37956585\n",
      "Iteration 222, loss = 0.37490653\n",
      "Iteration 223, loss = 0.37177618\n",
      "Iteration 224, loss = 0.37018175\n",
      "Iteration 225, loss = 0.36597805\n",
      "Iteration 226, loss = 0.36300841\n",
      "Iteration 227, loss = 0.35814849\n",
      "Iteration 228, loss = 0.35579980\n",
      "Iteration 229, loss = 0.35356145\n",
      "Iteration 230, loss = 0.34892140\n",
      "Iteration 231, loss = 0.34803407\n",
      "Iteration 232, loss = 0.34523008\n",
      "Iteration 233, loss = 0.34521570\n",
      "Iteration 234, loss = 0.34070518\n",
      "Iteration 235, loss = 0.33509876\n",
      "Iteration 236, loss = 0.33077642\n",
      "Iteration 237, loss = 0.32652651\n",
      "Iteration 238, loss = 0.32171491\n",
      "Iteration 239, loss = 0.32007973\n",
      "Iteration 240, loss = 0.31624944\n",
      "Iteration 241, loss = 0.31489636\n",
      "Iteration 242, loss = 0.31067964\n",
      "Iteration 243, loss = 0.30781200\n",
      "Iteration 244, loss = 0.30370158\n",
      "Iteration 245, loss = 0.30401864\n",
      "Iteration 246, loss = 0.30051811\n",
      "Iteration 247, loss = 0.29825450\n",
      "Iteration 248, loss = 0.29560931\n",
      "Iteration 249, loss = 0.29068946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 250, loss = 0.28482232\n",
      "Iteration 251, loss = 0.28170962\n",
      "Iteration 252, loss = 0.28142849\n",
      "Iteration 253, loss = 0.27709573\n",
      "Iteration 254, loss = 0.27391333\n",
      "Iteration 255, loss = 0.27335542\n",
      "Iteration 256, loss = 0.27033037\n",
      "Iteration 257, loss = 0.26989770\n",
      "Iteration 258, loss = 0.26470822\n",
      "Iteration 259, loss = 0.26152284\n",
      "Iteration 260, loss = 0.25708147\n",
      "Iteration 261, loss = 0.25526161\n",
      "Iteration 262, loss = 0.25191281\n",
      "Iteration 263, loss = 0.25059495\n",
      "Iteration 264, loss = 0.24779866\n",
      "Iteration 265, loss = 0.24327413\n",
      "Iteration 266, loss = 0.23962821\n",
      "Iteration 267, loss = 0.23775276\n",
      "Iteration 268, loss = 0.23582779\n",
      "Iteration 269, loss = 0.23220085\n",
      "Iteration 270, loss = 0.23084939\n",
      "Iteration 271, loss = 0.23031638\n",
      "Iteration 272, loss = 0.22936258\n",
      "Iteration 273, loss = 0.22703009\n",
      "Iteration 274, loss = 0.22614737\n",
      "Iteration 275, loss = 0.22118515\n",
      "Iteration 276, loss = 0.22043437\n",
      "Iteration 277, loss = 0.21559933\n",
      "Iteration 278, loss = 0.21168882\n",
      "Iteration 279, loss = 0.21000955\n",
      "Iteration 280, loss = 0.20943358\n",
      "Iteration 281, loss = 0.20478125\n",
      "Iteration 282, loss = 0.20359159\n",
      "Iteration 283, loss = 0.20377535\n",
      "Iteration 284, loss = 0.19705597\n",
      "Iteration 285, loss = 0.19671598\n",
      "Iteration 286, loss = 0.19296299\n",
      "Iteration 287, loss = 0.18921392\n",
      "Iteration 288, loss = 0.18684416\n",
      "Iteration 289, loss = 0.18565109\n",
      "Iteration 290, loss = 0.18262465\n",
      "Iteration 291, loss = 0.18033260\n",
      "Iteration 292, loss = 0.17873160\n",
      "Iteration 293, loss = 0.17518818\n",
      "Iteration 294, loss = 0.17403595\n",
      "Iteration 295, loss = 0.17513157\n",
      "Iteration 296, loss = 0.17270411\n",
      "Iteration 297, loss = 0.17060935\n",
      "Iteration 298, loss = 0.17066110\n",
      "Iteration 299, loss = 0.16709514\n",
      "Iteration 300, loss = 0.16348925\n",
      "Iteration 301, loss = 0.16083327\n",
      "Iteration 302, loss = 0.15999452\n",
      "Iteration 303, loss = 0.15641585\n",
      "Iteration 304, loss = 0.15578864\n",
      "Iteration 305, loss = 0.15337509\n",
      "Iteration 306, loss = 0.16846279\n",
      "Iteration 307, loss = 0.16444252\n",
      "Iteration 308, loss = 0.15490897\n",
      "Iteration 309, loss = 0.14884566\n",
      "Iteration 310, loss = 0.15058194\n",
      "Iteration 311, loss = 0.14599176\n",
      "Iteration 312, loss = 0.14362062\n",
      "Iteration 313, loss = 0.13848533\n",
      "Iteration 314, loss = 0.13592411\n",
      "Iteration 315, loss = 0.13265040\n",
      "Iteration 316, loss = 0.13107914\n",
      "Iteration 317, loss = 0.12963180\n",
      "Iteration 318, loss = 0.12893433\n",
      "Iteration 319, loss = 0.12686010\n",
      "Iteration 320, loss = 0.12617478\n",
      "Iteration 321, loss = 0.12339814\n",
      "Iteration 322, loss = 0.12281352\n",
      "Iteration 323, loss = 0.12134507\n",
      "Iteration 324, loss = 0.12254330\n",
      "Iteration 325, loss = 0.12290114\n",
      "Iteration 326, loss = 0.12322744\n",
      "Iteration 327, loss = 0.12501902\n",
      "Iteration 328, loss = 0.12796282\n",
      "Iteration 329, loss = 0.13116478\n",
      "Iteration 330, loss = 0.12509360\n",
      "Iteration 331, loss = 0.11921744\n",
      "Iteration 332, loss = 0.11191429\n",
      "Iteration 333, loss = 0.10763086\n",
      "Iteration 334, loss = 0.10414824\n",
      "Iteration 335, loss = 0.10302213\n",
      "Iteration 336, loss = 0.10045740\n",
      "Iteration 337, loss = 0.09950469\n",
      "Iteration 338, loss = 0.09778730\n",
      "Iteration 339, loss = 0.09654468\n",
      "Iteration 340, loss = 0.09517982\n",
      "Iteration 341, loss = 0.09430723\n",
      "Iteration 342, loss = 0.09430583\n",
      "Iteration 343, loss = 0.09442999\n",
      "Iteration 344, loss = 0.09231400\n",
      "Iteration 345, loss = 0.09046114\n",
      "Iteration 346, loss = 0.09087318\n",
      "Iteration 347, loss = 0.08955323\n",
      "Iteration 348, loss = 0.08740583\n",
      "Iteration 349, loss = 0.08579240\n",
      "Iteration 350, loss = 0.08396744\n",
      "Iteration 351, loss = 0.08369699\n",
      "Iteration 352, loss = 0.08327836\n",
      "Iteration 353, loss = 0.08279692\n",
      "Iteration 354, loss = 0.08428086\n",
      "Iteration 355, loss = 0.09136102\n",
      "Iteration 356, loss = 0.10319674\n",
      "Iteration 357, loss = 0.11454905\n",
      "Iteration 358, loss = 0.11067616\n",
      "Iteration 359, loss = 0.09631394\n",
      "Iteration 360, loss = 0.08784617\n",
      "Iteration 361, loss = 0.08092159\n",
      "Iteration 362, loss = 0.07310480\n",
      "Iteration 363, loss = 0.07025135\n",
      "Iteration 364, loss = 0.06842798\n",
      "Iteration 365, loss = 0.06708538\n",
      "Iteration 366, loss = 0.06632223\n",
      "Iteration 367, loss = 0.06538594\n",
      "Iteration 368, loss = 0.06433106\n",
      "Iteration 369, loss = 0.06380626\n",
      "Iteration 370, loss = 0.06308071\n",
      "Iteration 371, loss = 0.06215121\n",
      "Iteration 372, loss = 0.06183907\n",
      "Iteration 373, loss = 0.06081195\n",
      "Iteration 374, loss = 0.06038908\n",
      "Iteration 375, loss = 0.05943423\n",
      "Iteration 376, loss = 0.05919126\n",
      "Iteration 377, loss = 0.05849620\n",
      "Iteration 378, loss = 0.05770916\n",
      "Iteration 379, loss = 0.05683366\n",
      "Iteration 380, loss = 0.05680351\n",
      "Iteration 381, loss = 0.05949515\n",
      "Iteration 382, loss = 0.05793038\n",
      "Iteration 383, loss = 0.06189612\n",
      "Iteration 384, loss = 0.07958197\n",
      "Iteration 385, loss = 0.09664869\n",
      "Iteration 386, loss = 0.09310338\n",
      "Iteration 387, loss = 0.08045990\n",
      "Iteration 388, loss = 0.07222239\n",
      "Iteration 389, loss = 0.05819605\n",
      "Iteration 390, loss = 0.05220627\n",
      "Iteration 391, loss = 0.04947158\n",
      "Iteration 392, loss = 0.04801076\n",
      "Iteration 393, loss = 0.04704208\n",
      "Iteration 394, loss = 0.04604316\n",
      "Iteration 395, loss = 0.04517394\n",
      "Iteration 396, loss = 0.04461360\n",
      "Iteration 397, loss = 0.04410611\n",
      "Iteration 398, loss = 0.04372414\n",
      "Iteration 399, loss = 0.04317524\n",
      "Iteration 400, loss = 0.04274508\n",
      "Iteration 401, loss = 0.04223513\n",
      "Iteration 402, loss = 0.04173190\n",
      "Iteration 403, loss = 0.04149841\n",
      "Iteration 404, loss = 0.04118034\n",
      "Iteration 405, loss = 0.04102900\n",
      "Iteration 406, loss = 0.04490701\n",
      "Iteration 407, loss = 0.05593098\n",
      "Iteration 408, loss = 0.06070013\n",
      "Iteration 409, loss = 0.07909173\n",
      "Iteration 410, loss = 0.09388388\n",
      "Iteration 411, loss = 0.07963127\n",
      "Iteration 412, loss = 0.06840554\n",
      "Iteration 413, loss = 0.05420044\n",
      "Iteration 414, loss = 0.04283793\n",
      "Iteration 415, loss = 0.03870781\n",
      "Iteration 416, loss = 0.03708186\n",
      "Iteration 417, loss = 0.03594036\n",
      "Iteration 418, loss = 0.03507546\n",
      "Iteration 419, loss = 0.03453121\n",
      "Iteration 420, loss = 0.03401853\n",
      "Iteration 421, loss = 0.03362135\n",
      "Iteration 422, loss = 0.03309349\n",
      "Iteration 423, loss = 0.03265275\n",
      "Iteration 424, loss = 0.03216804\n",
      "Iteration 425, loss = 0.03179715\n",
      "Iteration 426, loss = 0.03164928\n",
      "Iteration 427, loss = 0.03132481\n",
      "Iteration 428, loss = 0.03090933\n",
      "Iteration 429, loss = 0.03078841\n",
      "Iteration 430, loss = 0.03057041\n",
      "Iteration 431, loss = 0.03009340\n",
      "Iteration 432, loss = 0.02990389\n",
      "Iteration 433, loss = 0.03184987\n",
      "Iteration 434, loss = 0.04120077\n",
      "Iteration 435, loss = 0.03879163\n",
      "Iteration 436, loss = 0.03977678\n",
      "Iteration 437, loss = 0.03932338\n",
      "Iteration 438, loss = 0.04259993\n",
      "Iteration 439, loss = 0.03979898\n",
      "Iteration 440, loss = 0.04197370\n",
      "Iteration 441, loss = 0.04283320\n",
      "Iteration 442, loss = 0.04324138\n",
      "Iteration 443, loss = 0.04996613\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "loss:  10.800007778330095\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "\n",
    "# 3_Fold cross validation\n",
    "n_splits = 3\n",
    "\n",
    "kf = KFold(n_splits=n_splits)\n",
    "kf.get_n_splits(train_features)\n",
    "\n",
    "log_Loss = 0.0\n",
    "\n",
    "for train_index, test_index in kf.split(train_features):\n",
    "    \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train_, X_val_ = train_features.iloc[train_index], train_features.iloc[test_index]\n",
    "    y_train_, y_val_ = df_target_s.iloc[train_index], df_target_s.iloc[test_index]\n",
    "    \n",
    "    X_train_, X_val_ = np.array(X_train_), np.array(X_val_)\n",
    "    y_train_, y_val_ = np.array(y_train_), np.array(y_val_)\n",
    "    \n",
    "    # fix for empty columns - LogisticRegression blows up otherwise \n",
    "    check_for_empty_cols = np.where(y_train_.sum(axis = 0) == 0)[0]\n",
    "    if len(check_for_empty_cols):\n",
    "        y_train_[0, check_for_empty_cols] = 1\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=1500, verbose=1)\n",
    "    clf.fit(X_train_, y_train_)\n",
    "    y_pred = clf.predict(X_val_)\n",
    "    \n",
    "    # calc averag log loss\n",
    "    loss += calc_log_loss(y_val_, y_pred)\n",
    "    print(\"loss: \", loss)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23810826590639605"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_log_loss(y_val_, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
